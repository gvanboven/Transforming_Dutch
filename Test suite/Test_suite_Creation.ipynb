{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "729bd87a",
   "metadata": {},
   "source": [
    "In this notebook, I use the CheckList framework to create a template-based evaluation set which includes 4 tests concerning pronoun-related capacities:\n",
    "* linking names and pronouns\n",
    "* the usage of multiple pronouns by an individual\n",
    "* distinguishing the singular and plural usage of hen\n",
    "* distinguishing the usage of die as a relative and a personal pronoun\n",
    "\n",
    "The suite is stored in the file `suite.pkl` and the individual sentences are stored in `suite.txt`. The evaluation is done in the notebook `Test_suite_Evaluation.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "df162f74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/hpc/uu_cs_nlpsoc/gvanboven/wl-coref\n"
     ]
    }
   ],
   "source": [
    "%cd /hpc/uu_cs_nlpsoc/gvanboven/wl-coref"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "13174806",
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "\n",
    "import jsonlines\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "\n",
    "from coref.config import Config\n",
    "from coref.const import Doc, Span\n",
    "from typing import List, TextIO, Dict, Callable, Type\n",
    "from collections import defaultdict\n",
    "\n",
    "import checklist\n",
    "from checklist.pred_wrapper import PredictorWrapper\n",
    "from checklist.editor import Editor\n",
    "from checklist.perturb import Perturb\n",
    "from checklist.test_types import MFT, INV, DIR\n",
    "from checklist.expect import Expect\n",
    "from checklist.test_suite import TestSuite\n",
    "import json\n",
    "\n",
    "from coref import CorefModel\n",
    "from coref.tokenizer_customization import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "040ac318",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_file = 'config.toml'\n",
    "experiment = 'xlm-roberta'\n",
    "input_file = 'sample_input.jsonlines'\n",
    "\n",
    "weights_debiased = 'data/model_checkpoints/xlm_gn_comb_fine_248/xlm-roberta_e26.pt'\n",
    "weights_regular = 'data/model_checkpoints/xlm_regular_248/xlm-roberta_e18.pt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e891658f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_doc(doc: dict, model: CorefModel) -> dict:\n",
    "    filter_func = TOKENIZER_FILTERS.get(model.config.bert_model,\n",
    "                                        lambda _: True)\n",
    "    token_map = TOKENIZER_MAPS.get(model.config.bert_model, {})\n",
    "\n",
    "    word2subword = []\n",
    "    subwords = []\n",
    "    word_id = []\n",
    "    for i, word in enumerate(doc[\"cased_words\"]):\n",
    "        tokenized_word = (token_map[word]\n",
    "                          if word in token_map\n",
    "                          else model.tokenizer.tokenize(word))\n",
    "        tokenized_word = list(filter(filter_func, tokenized_word))\n",
    "        word2subword.append((len(subwords), len(subwords) + len(tokenized_word)))\n",
    "        subwords.extend(tokenized_word)\n",
    "        word_id.extend([i] * len(tokenized_word))\n",
    "    doc[\"word2subword\"] = word2subword\n",
    "    doc[\"subwords\"] = subwords\n",
    "    doc[\"word_id\"] = word_id\n",
    "\n",
    "    doc[\"head2span\"] = []\n",
    "    if \"speaker\" not in doc:\n",
    "        doc[\"speaker\"] = [\"_\" for _ in doc[\"cased_words\"]]\n",
    "    doc[\"word_clusters\"] = []\n",
    "    doc[\"span_clusters\"] = []\n",
    "\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5836fa48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_predictions(doc: Doc,\n",
    "                clusters: List[List[Span]]):\n",
    "    \"\"\" Writes span/cluster information to f_obj, which is assumed to be a file\n",
    "    object open for writing \"\"\"\n",
    "    placeholder = \"  -\" * 7\n",
    "    doc_id = doc[\"document_id\"]\n",
    "    words = doc[\"cased_words\"]\n",
    "    sents = doc[\"sent_id\"]\n",
    "\n",
    "    max_word_len = max(len(w) for w in words)\n",
    "\n",
    "    starts = defaultdict(lambda: [])\n",
    "    ends = defaultdict(lambda: [])\n",
    "    single_word = defaultdict(lambda: [])\n",
    "\n",
    "    for cluster_id, cluster in enumerate(clusters):\n",
    "        for start, end in cluster:\n",
    "            if end - start == 1:\n",
    "                single_word[start].append(cluster_id)\n",
    "            else:\n",
    "                starts[start].append(cluster_id)\n",
    "                ends[end - 1].append(cluster_id)\n",
    "    cluster_list = []\n",
    "    word_number = 0\n",
    "    for word_id, word in enumerate(words):\n",
    "\n",
    "        cluster_info_lst = []\n",
    "        for cluster_marker in starts[word_id]:\n",
    "            cluster_info_lst.append(f\"({cluster_marker}\")\n",
    "        for cluster_marker in single_word[word_id]:\n",
    "            cluster_info_lst.append(f\"({cluster_marker})\")\n",
    "        for cluster_marker in ends[word_id]:\n",
    "            cluster_info_lst.append(f\"{cluster_marker})\")\n",
    "        cluster_info = \"|\".join(cluster_info_lst) if cluster_info_lst else \"-\"\n",
    "        cluster_list.append(cluster_info)\n",
    "\n",
    "        if word_id == 0 or sents[word_id] != sents[word_id - 1]:\n",
    "            word_number = 0\n",
    "\n",
    "        word_number += 1\n",
    "    \n",
    "    return {'words': words, 'preds': cluster_list}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc8bfa9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sent(sent: str):\n",
    "    tokens = sent.split()\n",
    "    data = {\n",
    "            \"document_id\": \"\",\n",
    "            \"cased_words\": tokens,\n",
    "            \"sent_id\": [0 for i in range(len(tokens))]\n",
    "    }\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bbbe9321",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(weights):\n",
    "        model = CorefModel(config_file, experiment, build_optimizers=False, lr=5e-4, bert_lr=3e-5)\n",
    "\n",
    "        model.load_weights(path=weights, map_location=\"cpu\",\n",
    "                           ignore={\"bert_optimizer\", \"general_optimizer\",\n",
    "                                  \"bert_scheduler\", \"general_scheduler\"})\n",
    "        model.training = False\n",
    "        return model\n",
    "\n",
    "def make_pred(model, input_data):\n",
    "    docs = [build_doc(doc, model) for doc in input_data]\n",
    "    \n",
    "    outputs = []\n",
    "    with torch.no_grad():\n",
    "        for doc in tqdm(docs, unit=\"docs\"):\n",
    "            result, _ = model.run(doc)\n",
    "            #print(result)\n",
    "            doc[\"span_clusters\"] = result.span_clusters\n",
    "            doc[\"word_clusters\"] = result.word_clusters\n",
    "\n",
    "            for key in (\"word2subword\", \"subwords\", \"word_id\", \"head2span\"):\n",
    "                del doc[key]\n",
    "            \n",
    "            #print(result.word_clusters)\n",
    "\n",
    "            output = return_predictions(doc, [[(i, i + 1) for i in cluster]\n",
    "                                           for cluster in result.word_clusters])\n",
    "            outputs.append(output)\n",
    "    return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cfaa6e83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_arg_2mentions(predictions: Dict, mention1: str, mention2: str) -> (str, str):\n",
    "    \"\"\" \n",
    "    Helper function to extract target arguments, when there are two targets\n",
    "    :param pred: the model prediction\n",
    "    :param mention1: the token of the first mention\n",
    "    :param mention2: the token of the second mention\n",
    "\n",
    "    :returns the predictions for the first and second mention\n",
    "\n",
    "    \"\"\"\n",
    "    words = predictions['words']\n",
    "    preds = predictions['preds']\n",
    "    \n",
    "    m1_idx = words.index(mention1)\n",
    "    m2_idx = words.index(mention2)\n",
    "    \n",
    "    return(preds[m1_idx], preds[m2_idx])\n",
    "\n",
    "def get_arg_3mentions(predictions: Dict, mention1: str, mention2: str, mention3: str) -> (str, str):\n",
    "    \"\"\" \n",
    "    Helper function to extract target arguments, when there are three targets\n",
    "    :param pred: the model prediction\n",
    "    :param mention1: the token of the first mention\n",
    "    :param mention2: the token of the second mention\n",
    "    :param mention3: the token of the thrid mention\n",
    "\n",
    "    :returns the predictions for the first, second and third mention\n",
    "\n",
    "    \"\"\"\n",
    "    words = predictions['words']\n",
    "    preds = predictions['preds']\n",
    "    \n",
    "    m1_idx = words.index(mention1)\n",
    "    m2_idx = words.index(mention2)\n",
    "    m3_idx = words.index(mention3)\n",
    "    \n",
    "    return(preds[m1_idx], preds[m2_idx], preds[m3_idx])\n",
    "\n",
    "def get_arg_4mentions(predictions: Dict, mention1: str, mention2: str, mention3: str, mention4: str) -> (str, str):\n",
    "    \"\"\" \n",
    "    Helper function to extract target arguments, when there are three targets\n",
    "    :param pred: the model prediction\n",
    "    :param mention1: the token of the first mention\n",
    "    :param mention2: the token of the second mention\n",
    "    :param mention3: the token of the thrid mention\n",
    "\n",
    "    :returns the predictions for the first, second and third mention\n",
    "\n",
    "    \"\"\"\n",
    "    words = predictions['words']\n",
    "    preds = predictions['preds']\n",
    "    \n",
    "    \n",
    "    m1_idx = words.index(mention1)\n",
    "    m2_idx = words.index(mention2)\n",
    "    m3_idx = words.index(mention3)\n",
    "    m4_idx = words.index(mention4)\n",
    "    \n",
    "    return(preds[m1_idx], preds[m2_idx], preds[m3_idx], preds[m4_idx])\n",
    "\n",
    "\n",
    "def get_model_results(model: Type[CorefModel], n : int, sentence : str, capability : str, testcase_name : str) -> Dict:\n",
    "    \"\"\"\n",
    "    Extracts the predictions of a given model for a given sentence\n",
    "    And returns a dict with this info, which is to be stored in an output file\n",
    "\n",
    "    :param model: a pretrained srl model\n",
    "    :param n: the number of the current data sample\n",
    "    :param sentence: the current data sample sentence\n",
    "    :param capability: the name of the linguistic capability of interest\n",
    "    :param testcase_name: the name of the current test\n",
    "\n",
    "    :returns results: dict with all relevant info to store in output file\n",
    "    \"\"\"\n",
    "    results = {'sentence' : sentence,\n",
    "               'capability' : capability,\n",
    "               'testcase_name' : testcase_name,\n",
    "               'preds' : model.results['preds'][n],\n",
    "               'confs': model.results['confs'][n],\n",
    "               'passed' : bool(model.results['passed'][n])}\n",
    "    return results\n",
    "\n",
    "def extract_data_and_predictions(t: Dict, capability: str, testcase_name: str, test_regular: Dict, \n",
    "                                 test_debiased: Dict, test_data: List,  regular_predictions: List, \n",
    "                                 debiased_predictions : List) -> (List, List, List):\n",
    "    \"\"\" \n",
    "    Function that extracts all relevant test data and predictions information that is to be stored in the output files\n",
    "\n",
    "    :param t: dict containing all test case information\n",
    "    :param capability: the name of the linguistic capability of interest\n",
    "    :param testcase_name: the name of the current test\n",
    "    :param test_regular: predictions for the first SRL model\n",
    "    :param test_debiased: predictions for the second SRL model\n",
    "    :param test_data: output list in which all test cases should be stored\n",
    "    :param regular_predictions: output list in which all predictions of the first model should be stored\n",
    "    :param debiased_predictions: output list in which all predictions of the second model should be stored\n",
    "\n",
    "    :returns test_data: output list to which new test cases are added\n",
    "    :returns regular_predictions: output list to which new predictions of the first model are added\n",
    "    :returns debiased_predictions: output list to which new predictions of the second model are added\n",
    "    \"\"\"\n",
    "    for n, sentence in enumerate(t['data']):\n",
    "        #extract input sentence info\n",
    "              \n",
    "        input_item = {'sentence' : sentence,\n",
    "                        'meta' : t['meta'][n],\n",
    "                        'capability' : capability,\n",
    "                        'testcase_name': testcase_name}\n",
    "        #extract predictions info for the two models\n",
    "        regular_prediction =  get_model_results(test_regular, n, sentence, capability, testcase_name)\n",
    "        debiased_prediction =  get_model_results(test_debiased, n, sentence, capability, testcase_name)\n",
    "        #save extracted info\n",
    "        test_data.append(input_item)\n",
    "        regular_predictions.append(regular_prediction)\n",
    "        debiased_predictions.append(debiased_prediction)\n",
    "    return test_data, regular_predictions, debiased_predictions\n",
    "\n",
    "\n",
    "def predict_and_store(t: Dict, capability: str, testcase_name: str, expect: Callable, formattype: Callable, \n",
    "                      predict_regular: Type[CorefModel], predict_debiased: Type[CorefModel], \\\n",
    "                      test_data: List, regular_predictions: List, debiased_predictions: List) -> (List, List, List):\n",
    "    \"\"\" \n",
    "    Function that creates test cases given a template, makes predictions for the given models and stores the test cases as \n",
    "    well as the predictions\n",
    "\n",
    "    :param t: dict containing all test case information\n",
    "    :param capability: the name of the linguistic capability of interest\n",
    "    :param testcase_name: the name of the current test\n",
    "    :param expect: function that checks if the argument of interest is predicted as expected\n",
    "    :param formattype: function that creates the correct formatting for the test\n",
    "    :param predict_regular: first pretrained SRL model to be tested\n",
    "    :param predict_srlbert: second pretrained SRL model to be tested\n",
    "    :param test_data: output list in which all test cases should be stored\n",
    "    :param regular_predictions: output list in which all predictions of the first model should be stored\n",
    "    :param debiased_predictions: output list in which all predictions of the second model should be stored\n",
    "\n",
    "    :returns test_data: output list to which new test cases are added\n",
    "    :returns regular_predictions: output list to which new predictions of the first model are added\n",
    "    :returns debiased_predictions: output list to which new predictions of the second model are added\n",
    "    \"\"\"\n",
    "    #test the srl model\n",
    "    print('regular model')\n",
    "    test_regular = MFT(**t, expect=expect)\n",
    "    test_regular.run(predict_regular)\n",
    "    test_regular.summary(format_example_fn=formattype)\n",
    "    \n",
    "    print('debiased model')\n",
    "    test_debiased = MFT(**t, expect=expect)\n",
    "    test_debiased.run(predict_debiased)\n",
    "    test_debiased.summary(format_example_fn=formattype)\n",
    "\n",
    "    #store samples and predictions\n",
    "    test_data, regular_predictions, debiased_predictions = extract_data_and_predictions(t, capability, testcase_name, \\\n",
    "                                                                                 test_regular, test_debiased, test_data, \\\n",
    "                                                                                 regular_predictions, debiased_predictions)\n",
    "    return test_data, regular_predictions, debiased_predictions\n",
    "\n",
    "def store_data(path: str, data: List, new_file: bool=True):\n",
    "    \"\"\"\n",
    "    Function that saves a given list to a json file on the given path\n",
    "\n",
    "    :param path: path to a .json file to store data in\n",
    "    :param data: list containing information to be stored\n",
    "    :param new_file: setting indicating whether previous information in the file should be deleted (True) or not (False)\n",
    "    \"\"\"\n",
    "    #if there already is content in the file, make sure we do not lose it. \n",
    "    if new_file == False:\n",
    "        with open(path, \"r\") as file:\n",
    "            old_data = json.load(file)\n",
    "\n",
    "        old_data.append(data)\n",
    "        data = old_data\n",
    "\n",
    "    with open(path, \"w\") as file:\n",
    "        json.dump(data, file, indent=4, sort_keys=True)\n",
    "\n",
    "\n",
    "# Helper function to display failures \n",
    "def format_sent(x, pred, conf, label=None, meta=None):\n",
    "\n",
    "    predicate_structure = [*zip(pred['words'], pred['preds'])]\n",
    "        \n",
    "    return predicate_structure\n",
    "\n",
    "\n",
    "def found_name_with_pronouns(x, pred, conf, label=None, meta=None):\n",
    "    \n",
    "    # the name and the pronoun should have the same cluster annotation\n",
    "    m1 = meta['pronoun']\n",
    "    m2 = meta['name']\n",
    "    \n",
    "    arg1 = get_arg_2mentions(pred, m1, m2)\n",
    "\n",
    "    if arg1[0] == arg1[1] and arg1[0]!= '-':\n",
    "        pass_ = True\n",
    "    else:\n",
    "        pass_ = False\n",
    "    return pass_\n",
    "\n",
    "def found_pronoun_with_pronoun(x, pred, conf, label=None, meta=None):\n",
    "    \n",
    "    # the two pronouns should have the same cluster annotations\n",
    "    m1 = meta['pronoun_subj']\n",
    "    m2 = meta['pronoun_poss']\n",
    "    \n",
    "    arg1 = get_arg_2mentions(pred, m1, m2)\n",
    "\n",
    "    if arg1[0] == arg1[1] and arg1[0]!= '-':\n",
    "        pass_ = True\n",
    "    else:\n",
    "        pass_ = False\n",
    "    return pass_\n",
    "\n",
    "\n",
    "def found_name_pronoun_pronoun(x, pred, conf, label=None, meta=None):\n",
    "    \n",
    "    m1 = meta['name']\n",
    "    m2 = meta['pronoun_poss']\n",
    "    m3 = meta['pronoun_subj']\n",
    "    \n",
    "    arg1 = get_arg_3mentions(pred, m1, m2, m3)\n",
    "    \n",
    "    #the three mentions should have the same non-zero cluster annotation\n",
    "    if arg1[0] == arg1[1]== arg1[2] and arg1[0]!= '-':\n",
    "        pass_ = True\n",
    "    else:\n",
    "        pass_ = False\n",
    "    return pass_\n",
    "\n",
    "def found_name_sing_plur(x, pred, conf, label=None, meta=None):\n",
    "    \n",
    "    # the first two sing mentions should be the same and the latter two plural mentions should be the same\n",
    "    m1 = meta['name']\n",
    "    m2 = meta['sport']+'team'\n",
    "    m3 = meta['pronoun_singular']\n",
    "    m4 = meta['pronoun_plural']\n",
    "    \n",
    "    words = pred['words']\n",
    "    predictions = pred['preds']\n",
    "    \n",
    "    \n",
    "    m1_idx = words.index(m1)\n",
    "    m2_idx = words.index(m2)\n",
    "    m3_idx = words.index(m3)\n",
    "    m4_idx = [idx for idx,w in enumerate(words) if w==m4][-1]#words.index(mention4)\n",
    "    \n",
    "    preds = predictions[m1_idx], predictions[m2_idx], predictions[m3_idx], predictions[m4_idx]\n",
    "\n",
    "    if preds[0] == preds[2] and preds[0]!= '-' and preds[1] == preds[3] and preds[1]!= '-' and preds[0]!= preds[1]:\n",
    "        pass_ = True\n",
    "    else:\n",
    "        pass_ = False\n",
    "    return pass_\n",
    "\n",
    "def found_name_plur_sing(x, pred, conf, label=None, meta=None):\n",
    "    \n",
    "    # the first two sing mentions should be the same and the latter two plural mentions should be the same\n",
    "    m1 = meta['name']\n",
    "    m2 = meta['sport']+'team'\n",
    "    m3 = meta['pronoun_plural']\n",
    "    m4 = meta['pronoun_singular']\n",
    "\n",
    "    \n",
    "    preds = get_arg_4mentions(pred, m1, m2, m3, m4)\n",
    "\n",
    "    if preds[0] == preds[3] and preds[0]!= '-' and preds[1] == preds[2] and preds[1]!= '-' and preds[0]!= preds[1]:\n",
    "        pass_ = True\n",
    "    else:\n",
    "        pass_ = False\n",
    "    return pass_\n",
    "\n",
    "def found_die_die(x, pred, conf, label=None, meta=None):\n",
    "    \n",
    "    # the first two sing mentions should be the same and the latter two plural mentions should be the same\n",
    "    m1 = meta['name']\n",
    "    m2 = meta['die']\n",
    "    m3 = meta['pronoun_subj']\n",
    "\n",
    "    words = pred['words']\n",
    "    predictions = pred['preds']\n",
    "    \n",
    "    m1_idx = words.index(m1)\n",
    "    m2_idx = words.index(m2)\n",
    "    m3_idx = [idx for idx,w in enumerate(words) if w==m3][-1]\n",
    "    \n",
    "    preds = predictions[m1_idx], predictions[m2_idx], predictions[m3_idx]\n",
    "    \n",
    "\n",
    "    if preds[0] == preds[2] and preds[0]!= '-' and preds[1]== '-':\n",
    "        pass_ = True\n",
    "    else:\n",
    "        pass_ = False\n",
    "    return pass_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5acc777f",
   "metadata": {},
   "outputs": [],
   "source": [
    "editor = Editor()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7509e451",
   "metadata": {},
   "source": [
    "## First names + pronoun recognition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "3fa5918d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../eval/gn_names.txt\", \"r\") as namelist:\n",
    "    gn_names = namelist.read().split()\n",
    "pronouns_poss = ['zijn', 'haar', 'diens', 'hun']\n",
    "pronouns_poss_gendered = ['zijn', 'haar']\n",
    "pronouns_poss_gn = ['diens', 'hun']\n",
    "pronouns_subj = ['hij', 'zij', 'die', 'hen']\n",
    "pronouns_subj_gendered = ['hij', 'zij']\n",
    "pronouns_subj_gn = ['die', 'hen']\n",
    "pronouns_obj = ['hem', 'haar', 'hen', 'hen']\n",
    "pronouns_obj_gendered = ['hem', 'haar']\n",
    "pronouns_obj_gn= ['hen', 'hen']\n",
    "\n",
    "testcase_name = 'gn_names'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "dd6be783",
   "metadata": {},
   "outputs": [],
   "source": [
    "expect_name_pronouns = Expect.single(found_name_with_pronouns)\n",
    "expect_pronoun_pronoun = Expect.single(found_pronoun_with_pronoun)\n",
    "expect_name_pronoun_pronoun = Expect.single(found_name_pronoun_pronoun)\n",
    "expect_sing_plur = Expect.single(found_name_sing_plur)\n",
    "expect_plur_sing = Expect.single(found_name_plur_sing)\n",
    "expect_die_die = Expect.single(found_die_die)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "7ab52c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "suite = TestSuite()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8238d8f",
   "metadata": {},
   "source": [
    "### gender neutral names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "d3a56d29",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "capability = 'Name pronoun link'\n",
    "test_data = []\n",
    "\n",
    "regular_predictions = []\n",
    "debiased_predictions = []\n",
    "\n",
    "#create samples\n",
    "testcase_name = 'gn_names'\n",
    "t = editor.template('{name} gaat naar {pronoun} {place}.',\n",
    "                       name=gn_names, \n",
    "                       pronoun=pronouns_poss_gendered, \n",
    "                       place=['werk', 'dokter', 'afspraak', 'buren', 'ouders', 'interview'], meta=True, \n",
    "                        nsamples=100)\n",
    "\n",
    "name_pronoun_gn_g = MFT(**t, name='Name pronoun, gender neutral name, gendered pronouns', \\\n",
    "                      description='Linking a pronoun with a name, using gender neutral names and gendered pronouns', \\\n",
    "                      expect=expect_name_pronouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "634a0444",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "test_data = []\n",
    "\n",
    "regular_predictions = []\n",
    "debiased_predictions = []\n",
    "\n",
    "#create samples\n",
    "testcase_name = 'gn_names'\n",
    "t = editor.template('{name} gaat naar {pronoun} {place}.',\n",
    "                       name=gn_names, \n",
    "                       pronoun=pronouns_poss_gn, \n",
    "                       place=['werk', 'dokter', 'afspraak', 'buren', 'ouders', 'interview'], meta=True, \n",
    "                        nsamples=100)\n",
    "\n",
    "name_pronoun_gn_gn = MFT(**t, name='Name pronoun, gender neutral name, gender neutral pronouns', \\\n",
    "                      description='Linking a pronoun with a name, using gender neutral names and gender neutral pronouns', \\\n",
    "                      expect=expect_name_pronouns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a17ef6e",
   "metadata": {},
   "source": [
    "### male names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "5da941a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = []\n",
    "\n",
    "regular_predictions = []\n",
    "debiased_predictions = []\n",
    "\n",
    "t = editor.template('{name} gaat naar {pronoun} {place}.',\n",
    "                       name=editor.lexicons.male_from['the_Netherlands'], \n",
    "                       pronoun=pronouns_poss_gendered, \n",
    "                       place=['werk', 'dokter', 'afspraak', 'buren', 'ouders', 'interview'], meta=True, \n",
    "                        nsamples=100)\n",
    "\n",
    "name_pronoun_m_g = MFT(**t, name='Name pronoun, male name, gendered pronouns', \\\n",
    "                      description='Linking a pronoun with a name, using male names and gendered pronouns', \\\n",
    "                      expect=expect_name_pronouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "30d78d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = []\n",
    "\n",
    "regular_predictions = []\n",
    "debiased_predictions = []\n",
    "\n",
    "t = editor.template('{name} gaat naar {pronoun} {place}.',\n",
    "                       name=editor.lexicons.male_from['the_Netherlands'], \n",
    "                       pronoun=pronouns_poss_gn, \n",
    "                       place=['werk', 'dokter', 'afspraak', 'buren', 'ouders', 'interview'], meta=True, \n",
    "                        nsamples=100)\n",
    "\n",
    "name_pronoun_m_gn = MFT(**t, name='Name pronoun, male name, gender neutral pronouns', \\\n",
    "                      description='Linking a pronoun with a name, using male names and gender neutral pronouns', \\\n",
    "                      expect=expect_name_pronouns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98051e9d",
   "metadata": {},
   "source": [
    "### female names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "a0eaec0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = []\n",
    "\n",
    "regular_predictions = []\n",
    "debiased_predictions = []\n",
    "\n",
    "t = editor.template('{name} gaat naar {pronoun} {place}.',\n",
    "                       name=editor.lexicons.female_from['the_Netherlands'], \n",
    "                       pronoun=pronouns_poss_gendered, \n",
    "                       place=['werk', 'dokter', 'afspraak', 'buren', 'ouders', 'interview'], meta=True, \n",
    "                        nsamples=100)\n",
    "\n",
    "name_pronoun_f_g = MFT(**t, name='Name pronoun, female name, gendered pronouns', \\\n",
    "                      description='Linking a pronoun with a name, using female names and gendered pronouns', \\\n",
    "                      expect=expect_name_pronouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "638ca521",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = []\n",
    "\n",
    "regular_predictions = []\n",
    "debiased_predictions = []\n",
    "\n",
    "t = editor.template('{name} gaat naar {pronoun} {place}.',\n",
    "                       name=editor.lexicons.female_from['the_Netherlands'], \n",
    "                       pronoun=pronouns_poss_gn, \n",
    "                       place=['werk', 'dokter', 'afspraak', 'buren', 'ouders', 'interview'], meta=True, \n",
    "                        nsamples=100)\n",
    "\n",
    "name_pronoun_f_gn = MFT(**t, name='Name pronoun, female name, gender neutral pronouns', \\\n",
    "                      description='Linking a pronoun with a name, using female names and gender neutral pronouns', \\\n",
    "                      expect=expect_name_pronouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "7035db13",
   "metadata": {},
   "outputs": [],
   "source": [
    "suite.add(name_pronoun_gn_g, capability=capability)\n",
    "suite.add(name_pronoun_gn_gn, capability=capability)\n",
    "suite.add(name_pronoun_m_g, capability=capability)\n",
    "suite.add(name_pronoun_m_gn, capability=capability)\n",
    "suite.add(name_pronoun_f_g, capability=capability)\n",
    "suite.add(name_pronoun_f_gn, capability=capability)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4cad0d5",
   "metadata": {},
   "source": [
    "## Multiple pronouns per entity\n",
    "### short sentence with only pronouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "ddf1e7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "capability = 'pronoun-pronoun'\n",
    "test_data = []\n",
    "\n",
    "regular_predictions = []\n",
    "debiased_predictions = []\n",
    "\n",
    "\n",
    "t = editor.template('{pronoun_subj} gaat naar {pronoun_poss} {place}.',\n",
    "                       pronoun_subj = pronouns_subj_gendered, \n",
    "                       pronoun_poss=pronouns_poss_gendered, \n",
    "                       place=['werk', 'dokter', 'afspraak', 'buren', 'ouders', 'interview'], meta=True, \n",
    "                       nsamples=100)\n",
    "pronoun_pronoun_g = MFT(**t, name='Pronoun pronoun, gendered pronouns', \\\n",
    "                      description='Linking a pronoun with a pronoun, using gendered pronouns', \\\n",
    "                      expect=expect_pronoun_pronoun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "b27123ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = []\n",
    "\n",
    "regular_predictions = []\n",
    "debiased_predictions = []\n",
    "\n",
    "np.random.seed(2020)\n",
    "t = editor.template('{pronoun_subj} gaat naar {pronoun_poss} {place}.',\n",
    "                       pronoun_subj = pronouns_subj_gn, \n",
    "                       pronoun_poss=pronouns_poss_gn, \n",
    "                       place=['werk', 'dokter', 'afspraak', 'buren', 'ouders', 'interview'], meta=True, \n",
    "                       nsamples=100)\n",
    "pronoun_pronoun_gn = MFT(**t, name='Pronoun pronoun, gender neutral pronouns', \\\n",
    "                      description='Linking a pronoun with a pronoun, using gender neutral pronouns', \\\n",
    "                      expect=expect_pronoun_pronoun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "91541873",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = []\n",
    "\n",
    "regular_predictions = []\n",
    "debiased_predictions = []\n",
    "\n",
    "t = editor.template('{pronoun_subj} gaat naar {pronoun_poss} {place}.',\n",
    "                       pronoun_subj = pronouns_subj, \n",
    "                       pronoun_poss=pronouns_poss, \n",
    "                       place=['werk', 'dokter', 'afspraak', 'buren', 'ouders', 'interview'], meta=True, \n",
    "                       nsamples=100)\n",
    "pronoun_pronoun_mix = MFT(**t, name='Pronoun pronoun, mixed pronouns', \\\n",
    "                      description='Linking a pronoun with a pronoun, using mixed pronouns', \\\n",
    "                      expect=expect_pronoun_pronoun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "2e1e894d",
   "metadata": {},
   "outputs": [],
   "source": [
    "suite.add(pronoun_pronoun_g, capability=capability)\n",
    "suite.add(pronoun_pronoun_gn, capability=capability)\n",
    "suite.add(pronoun_pronoun_mix, capability=capability)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1697bccf",
   "metadata": {},
   "source": [
    "### longer sentence with a name and different pronouns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "8bb1589b",
   "metadata": {},
   "outputs": [],
   "source": [
    "capability = 'pronoun-pronoun-name'\n",
    "test_data = []\n",
    "\n",
    "regular_predictions = []\n",
    "debiased_predictions = []\n",
    "\n",
    "t = editor.template('{name} heeft {pronoun_poss} afspraak bij de {place} om 3 uur. {pronoun_subj} gaat zo heen.',\n",
    "                       name=gn_names,\n",
    "                       pronoun_subj = pronouns_subj_gendered, \n",
    "                       pronoun_poss=pronouns_poss_gendered, \n",
    "                       place=['dokters', 'huisartsenpost', 'buren', 'gemeente', 'winkel', 'sportschool'], meta=True, \n",
    "                       nsamples=100)\n",
    "\n",
    "pronoun_pronoun_name_g = MFT(**t, name='Pronoun pronoun name, gendered pronouns', \\\n",
    "                      description='Linking a pronoun with a pronoun and a name, using gendered pronouns', \\\n",
    "                      expect=expect_name_pronoun_pronoun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "67d2456b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = []\n",
    "\n",
    "regular_predictions = []\n",
    "debiased_predictions = []\n",
    "\n",
    "\n",
    "t = editor.template('{name} heeft {pronoun_poss} afspraak bij de {place} om 3 uur. {pronoun_subj} gaat zo heen.',\n",
    "                       name=gn_names,\n",
    "                       pronoun_subj = pronouns_subj_gn, \n",
    "                       pronoun_poss=pronouns_poss_gn, \n",
    "                       place=['dokters', 'huisartsenpost', 'buren', 'gemeente', 'winkel', 'sportschool'], meta=True, \n",
    "                       nsamples=100)\n",
    "\n",
    "pronoun_pronoun_name_gn = MFT(**t, name='Pronoun pronoun name, gender neutral pronouns', \\\n",
    "                      description='Linking a pronoun with a pronoun and a name, using gender neutral pronouns', \\\n",
    "                      expect=expect_name_pronoun_pronoun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "494540b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = []\n",
    "\n",
    "regular_predictions = []\n",
    "debiased_predictions = []\n",
    "\n",
    "t = editor.template('{name} heeft {pronoun_poss} afspraak bij de {place} om 3 uur. {pronoun_subj} gaat zo heen.',\n",
    "                       name=gn_names,\n",
    "                       pronoun_subj = pronouns_subj, \n",
    "                       pronoun_poss=pronouns_poss, \n",
    "                       place=['dokters', 'huisartsenpost', 'buren', 'gemeente', 'winkel', 'sportschool'], meta=True, \n",
    "                       nsamples=100)\n",
    "\n",
    "pronoun_pronoun_name_mix = MFT(**t, name='Pronoun pronoun name, mixed pronouns', \\\n",
    "                      description='Linking a pronoun with a pronoun and a name, using mixed pronouns', \\\n",
    "                      expect=expect_name_pronoun_pronoun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "32bf25a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "suite.add(pronoun_pronoun_name_g, capability=capability)\n",
    "suite.add(pronoun_pronoun_name_gn, capability=capability)\n",
    "suite.add(pronoun_pronoun_name_mix, capability=capability)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b749dda2",
   "metadata": {},
   "source": [
    "## singular plural distinction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "d451e39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "capability = 'hen-hen-amb'\n",
    "test_data = []\n",
    "\n",
    "regular_predictions = []\n",
    "debiased_predictions = []\n",
    "\n",
    "t = editor.template('{name} gaat met het {sport}team op vakantie. {pronoun_singular} is al vaker met {pronoun_plural} weggeweest.',\n",
    "                       name = gn_names, \n",
    "                       pronoun_singular=pronouns_subj_gendered,\n",
    "                       pronoun_plural = ['hen'],\n",
    "                       sport=['voetbal', 'volleybal', 'hockey', 'handbal', 'softbal', 'honkbal', 'basketbal'], meta=True, \n",
    "                       nsamples=100)\n",
    "\n",
    "\n",
    "hen_hen_p_g = MFT(**t, name='Hen-hen ambiguity, hen plural, gendered pronouns', \\\n",
    "                      description='Distinguishing the plural and singular usage of hen, using gendered pronouns', \\\n",
    "                      expect=expect_sing_plur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "22e2d3f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = []\n",
    "\n",
    "regular_predictions = []\n",
    "debiased_predictions = []\n",
    "\n",
    "t = editor.template('{name} gaat met het {sport}team op vakantie. {pronoun_singular} is al vaker met {pronoun_plural} weggeweest.',\n",
    "                       name = gn_names, \n",
    "                       pronoun_singular=pronouns_subj_gn,\n",
    "                       pronoun_plural = ['hen'],\n",
    "                       sport=['voetbal', 'volleybal', 'hockey', 'handbal', 'softbal', 'honkbal', 'basketbal'], meta=True, \n",
    "                       nsamples=100)\n",
    "\n",
    "hen_hen_p_gn = MFT(**t, name='Hen-hen ambiguity, hen plural, gender neutral pronouns', \\\n",
    "                      description='Distinguishing the plural and singular usage of hen, using gender neutral pronouns', \\\n",
    "                      expect=expect_sing_plur)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "d610d7bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = []\n",
    "\n",
    "regular_predictions = []\n",
    "debiased_predictions = []\n",
    "\n",
    "t = editor.template('{name} gaat met het {sport}team op vakantie. {pronoun_plural} zijn al vaker met {pronoun_singular} weggeweest .',\n",
    "                       name = gn_names, \n",
    "                       pronoun_singular=pronouns_obj_gendered, \n",
    "                       pronoun_plural = ['Zij'],\n",
    "                       sport=['voetbal', 'volleybal', 'hockey', 'handbal', 'softbal', 'honkbal', 'basketbal'], meta=True, \n",
    "                       nsamples=100, )\n",
    "\n",
    "hen_hen_s_g = MFT(**t, name='Hen-hen ambiguity, hen singular, gendered pronouns', \\\n",
    "                      description='Distinguishing the plural and singular usage of hen, using gendered pronouns', \\\n",
    "                      expect=expect_plur_sing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "5747bb66",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = []\n",
    "\n",
    "regular_predictions = []\n",
    "debiased_predictions = []\n",
    "\n",
    "t = editor.template('{name} gaat met het {sport}team op vakantie. {pronoun_plural} zijn al vaker met {pronoun_singular} weggeweest .',\n",
    "                       name = gn_names, \n",
    "                       pronoun_singular=pronouns_obj_gn, \n",
    "                       pronoun_plural = ['Zij'],\n",
    "                       sport=['voetbal', 'volleybal', 'hockey', 'handbal', 'softbal', 'honkbal', 'basketbal'], meta=True, \n",
    "                       nsamples=100, )\n",
    "\n",
    "hen_hen_s_gn = MFT(**t, name='Hen-hen ambiguity, hen singular, gender neutral pronouns', \\\n",
    "                      description='Distinguishing the plural and singular usage of hen, using gender neutral pronouns', \\\n",
    "                      expect=expect_plur_sing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "e26a8a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "suite.add(hen_hen_p_g, capability=capability)\n",
    "suite.add(hen_hen_p_gn, capability=capability)\n",
    "suite.add(hen_hen_s_g, capability=capability)\n",
    "suite.add(hen_hen_s_gn, capability=capability)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8663672d",
   "metadata": {},
   "source": [
    "## die/die distinction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "0b6a33d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = []\n",
    "\n",
    "regular_predictions = []\n",
    "debiased_predictions = []\n",
    "\n",
    "\n",
    "t = editor.template('{name} {die} hier net werkt is te laat omdat {pronoun_subj} een lekke band had .',\n",
    "                       name = gn_names, \n",
    "                       pronoun_subj=pronouns_subj_gendered, \n",
    "                       die=['die'],\n",
    "                       meta=True, \n",
    "                       nsamples=100)\n",
    "\n",
    "die_die_g = MFT(**t, name='Die-die ambiguity, gendered pronouns', \\\n",
    "                      description='Distinguishing two usages of die, using gendered pronouns', \\\n",
    "                      expect=expect_die_die)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "1bf19163",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = []\n",
    "\n",
    "regular_predictions = []\n",
    "debiased_predictions = []\n",
    "\n",
    "t = editor.template('{name} {die} hier net werkt is te laat omdat {pronoun_subj} een lekke band had .',\n",
    "                       name = gn_names, \n",
    "                       pronoun_subj=['die'], \n",
    "                       die=['die'],\n",
    "                       meta=True, \n",
    "                       nsamples=100)\n",
    "\n",
    "die_die_gn = MFT(**t, name='Die-die ambiguity, gender neutral pronouns', \\\n",
    "                      description='Distinguishing two usages of die, using gender neutral pronouns', \\\n",
    "                      expect=expect_die_die)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "53d03876",
   "metadata": {},
   "outputs": [],
   "source": [
    "suite.add(die_die_g, capability=capability)\n",
    "suite.add(die_die_gn, capability=capability)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1404e36a",
   "metadata": {},
   "source": [
    "# Storing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "40f3cf91",
   "metadata": {},
   "outputs": [],
   "source": [
    "suite.to_raw_file(\"suite.txt\")\n",
    "\n",
    "for test in suite.tests:\n",
    "    suite.tests[test].name = test\n",
    "    suite.tests[test].description = suite.info[test]['description]']\n",
    "    suite.tests[test].capability = suite.info[test]['capability']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "536ed96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"suite.pkl\"\n",
    "suite.save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcae38f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "wl-coref",
   "language": "python",
   "name": "wl-coref"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
